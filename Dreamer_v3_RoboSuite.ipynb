{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMG+/7SQaweiTfHuvsCC0el",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarStryke21/NaturalDreamer/blob/main/Dreamer_v3_RoboSuite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7pGBxXoixhv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Clone the Natural Dreamer repository\n",
        "\n",
        "!git clone https://github.com/FarStryke21/NaturalDreamer.git\n",
        "\n",
        "%cd /content/NaturalDreamer/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install robosuite attridict"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eLYRcvCPkzDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym # For wrappers\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Robosuite imports\n",
        "import robosuite as suite\n",
        "from robosuite.wrappers import GymWrapper\n",
        "\n",
        "# Project-specific imports\n",
        "from dreamer import Dreamer\n",
        "from utils import loadConfig, seedEverything, plotMetrics, saveLossesToCSV, ensureParentFolders\n",
        "from envs import getEnvProperties, GymPixelsProcessingWrapper, CleanGymWrapper, ImageExtractWrapper # Using getEnvProperties from envs.py\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WCbX6P2XklGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "UCqdB28Blmzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load experiment config\n",
        "configFile = \"robosuite-lift-panda.yml\"\n",
        "config = loadConfig(configFile)"
      ],
      "metadata": {
        "id": "koH01dgMl7ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed through the environment\n",
        "seedEverything(config.seed)\n",
        "print(f\"Loaded config from file: {configFile}\")\n",
        "print(f\"Using seed: {config.seed}\")"
      ],
      "metadata": {
        "id": "sWnvbQ0YmiHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup files and folder logging\n",
        "\n",
        "runName = f\"{config.environmentName}_{config.robotName}_{config.runName}\"\n",
        "\n",
        "# Define paths for checkpoints, metrics, plots, and videos\n",
        "checkpointFolder = os.path.join(config.folderNames.checkpointsFolder, runName)\n",
        "metricsFilename = os.path.join(config.folderNames.metricsFolder, f\"{runName}.csv\")\n",
        "plotFilename = os.path.join(config.folderNames.plotsFolder, f\"{runName}.html\")\n",
        "checkpointFilenameBase = os.path.join(checkpointFolder, runName) # Checkpoints will be saved in a subfolder per run\n",
        "videoFilenameBase = os.path.join(config.folderNames.videosFolder, runName)\n",
        "\n",
        "# Ensure parent directories for all output files exist\n",
        "ensureParentFolders(metricsFilename, plotFilename, checkpointFilenameBase, videoFilenameBase)\n",
        "print(f\"Run name: {runName}\")\n",
        "print(f\"Metrics will be saved to: {metricsFilename}\")\n",
        "print(f\"Plots will be saved to: {plotFilename}\")\n",
        "print(f\"Checkpoints will be saved to: {checkpointFolder}\")\n",
        "print(f\"Videos will be saved to: {config.folderNames.videosFolder}\")\n"
      ],
      "metadata": {
        "id": "Hs8T4Z7emwJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Robosuite training environment...\")\n",
        "robosuite_env_train = suite.make(\n",
        "    env_name=config.environmentName,\n",
        "    robots=config.robotName,\n",
        "    controller_configs=suite.load_composite_controller_config(controller=config.controllerName),\n",
        "    has_renderer=False,  # No on-screen rendering for training\n",
        "    has_offscreen_renderer=config.useCameraObs, # True if using camera observations for the encoder\n",
        "    use_camera_obs=config.useCameraObs,\n",
        "    camera_names=config.cameraName,\n",
        "    camera_heights=config.cameraHeight, # Robosuite will render at this resolution\n",
        "    camera_widths=config.cameraWidth,\n",
        "    reward_shaping=config.rewardShaping,\n",
        "    control_freq=config.controlFreq,\n",
        "    horizon=config.horizon,\n",
        ")\n",
        "\n",
        "train_env_keys = [f\"{config.cameraName}_image\"]\n",
        "env = GymWrapper(robosuite_env_train, keys=train_env_keys, flatten_obs=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "thJZlGPHnE4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply observation wrappers\n",
        "print(f\"Original Observation Space: {env.observation_space}\")\n",
        "env = ImageExtractWrapper(env, image_key=train_env_keys[0]) # Convert Dictionary Observation to Image Observation -> Box(0, 255, (64, 64, 3), uint8)\n",
        "env = gym.wrappers.ResizeObservation(env, (config.cameraHeight, config.cameraWidth)) # Resize image observation size -> Box(0, 255, (64, 64, 3), uint8)\n",
        "env = GymPixelsProcessingWrapper(env) # Rearrange Observation space -> Box(0.0, 1.0, (3, 64, 64), float32\n",
        "env = CleanGymWrapper(env) # Clean Gym Wrapper -> Box(0.0, 1.0, (3, 64, 64), float32)\n",
        "print(f\"New Observation Space{env.observation_space}\")\n",
        "print(\"Robosuite training environment initialized.\")"
      ],
      "metadata": {
        "id": "6NsXsmz_oMt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Robosuite evaluation environment...\")\n",
        "robosuite_env_eval = suite.make(\n",
        "    env_name=config.environmentName,\n",
        "    robots=config.robotName,\n",
        "    controller_configs=suite.load_composite_controller_config(controller=config.controllerName),\n",
        "    has_renderer=config.evaluationRender,  # Enable for video saving if needed\n",
        "    has_offscreen_renderer=config.useCameraObs or config.evaluationRender, # Offscreen needed for rgb_array for video\n",
        "    use_camera_obs=config.useCameraObs,\n",
        "    camera_names=config.cameraName,\n",
        "    camera_heights=config.cameraHeight,\n",
        "    camera_widths=config.cameraWidth,\n",
        "    reward_shaping=config.rewardShaping, # Usually good to keep consistent with training\n",
        "    control_freq=config.controlFreq,\n",
        "    horizon=config.horizon,\n",
        "    # render_camera=config.cameraName # Or a different camera like \"frontview\" for videos\n",
        ")\n",
        "eval_env_keys = [f\"{config.cameraName}_image\"]\n",
        "envEvaluation = GymWrapper(robosuite_env_eval, keys=eval_env_keys, flatten_obs=False)\n"
      ],
      "metadata": {
        "id": "uCkjN6Tjxm93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply observation wrappers\n",
        "print(f\"Original Observation Space: {envEvaluation.observation_space}\")\n",
        "envEvaluation = ImageExtractWrapper(envEvaluation, image_key=train_env_keys[0]) # Convert Dictionary Observation to Image Observation -> Box(0, 255, (64, 64, 3), uint8)\n",
        "envEvaluation = gym.wrappers.ResizeObservation(envEvaluation, (config.cameraHeight, config.cameraWidth)) # Resize image observation size -> Box(0, 255, (64, 64, 3), uint8)\n",
        "envEvaluation = GymPixelsProcessingWrapper(envEvaluation) # Rearrange Observation space -> Box(0.0, 1.0, (3, 64, 64), float32\n",
        "envEvaluation = CleanGymWrapper(envEvaluation) # Clean Gym Wrapper -> Box(0.0, 1.0, (3, 64, 64), float32)\n",
        "print(f\"New Observation Space{envEvaluation.observation_space}\")\n",
        "print(\"Robosuite training environment initialized.\")"
      ],
      "metadata": {
        "id": "GQThmG-xx9Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function should work with the wrapped environment\n",
        "observationShape, actionSize, actionLow, actionHigh = getEnvProperties(env)\n",
        "print(f\"Environment Properties: Observation Shape {observationShape}, Action Size {actionSize}, Action Low {actionLow}, Action High {actionHigh}\")\n",
        "if not config.useCameraObs:\n",
        "    print(\"Warning: 'useCameraObs' is False. The current Dreamer implementation primarily expects image observations for its Encoder. Ensure your network setup is appropriate for state-based observations if this is intended.\")"
      ],
      "metadata": {
        "id": "PLB9IZb7yOY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Dreamer agent...\")\n",
        "dreamer = Dreamer(observationShape, actionSize, actionLow, actionHigh, device, config.dreamer)"
      ],
      "metadata": {
        "id": "4a8M7024yZas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training from Scratch...\")\n",
        "dreamer.environmentInteraction(env, config.episodesBeforeStart, seed=config.seed)\n",
        "print(f\"Buffer size after initial collection: {len(dreamer.buffer)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VZiVVQL3yide"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterationsNum = config.gradientSteps // config.replayRatio\n",
        "print(f\"Starting training for {config.gradientSteps} gradient steps, with {iterationsNum} outer iterations.\")\n",
        "\n",
        "for iter_idx in tqdm(range(iterationsNum), desc=\"Training Progress\"):\n",
        "    # Inner loop for gradient updates\n",
        "\n",
        "    for _ in range(config.replayRatio):\n",
        "\n",
        "        if len(dreamer.buffer) < dreamer.config.batchSize * dreamer.config.batchLength: # Ensure enough data for a full sequence batch\n",
        "            print(f\"Buffer has {len(dreamer.buffer)} samples, less than required {dreamer.config.batchSize * dreamer.config.batchLength}. Collecting more...\")\n",
        "            dreamer.environmentInteraction(env, 1, seed=(config.seed + dreamer.totalEpisodes if config.seed else None)) # Collect one more episode\n",
        "\n",
        "            if len(dreamer.buffer) < dreamer.config.batchSize * dreamer.config.batchLength:\n",
        "                continue # Skip training step if still not enough data\n",
        "\n",
        "        sampledData = dreamer.buffer.sample(dreamer.config.batchSize, dreamer.config.batchLength)\n",
        "        initialStates, worldModelMetrics = dreamer.worldModelTraining(sampledData)\n",
        "        behaviorMetrics = dreamer.behaviorTraining(initialStates)\n",
        "        dreamer.totalGradientSteps += 1\n",
        "\n",
        "        # Checkpoint saving and evaluation\n",
        "        if dreamer.totalGradientSteps % config.checkpointInterval == 0 and config.saveCheckpoints:\n",
        "            suffix = f\"{dreamer.totalGradientSteps // 1000:.0f}k\"\n",
        "            currentCheckpointPath = f\"{checkpointFilenameBase}_{suffix}.pth\"\n",
        "            dreamer.saveCheckpoint(currentCheckpointPath)\n",
        "\n",
        "            # Perform evaluation\n",
        "            video_path_suffix = f\"{runName}_{suffix}\"\n",
        "            evaluationScore = dreamer.environmentInteraction(\n",
        "                envEvaluation,\n",
        "                config.numEvaluationEpisodes,\n",
        "                seed=(config.seed + dreamer.totalEpisodes if config.seed else None), # Use a different seed for eval if desired\n",
        "                evaluation=True,\n",
        "                saveVideo=True,\n",
        "                filename=os.path.join(config.folderNames.videosFolder, video_path_suffix) # Pass full path\n",
        "            )\n",
        "            print(f\"Iter {iter_idx+1}/{iterationsNum} | Grad Steps {dreamer.totalGradientSteps} | Saved Checkpoint: {currentCheckpointPath} | Eval Score: {evaluationScore if evaluationScore is not None else 'N/A':>8.2f}\")\n",
        "\n",
        "    # Collect more environment interactions\n",
        "    mostRecentScore = dreamer.environmentInteraction(env, config.numInteractionEpisodes, seed=(config.seed + dreamer.totalEpisodes if config.seed else None))\n",
        "\n",
        "    # Save metrics\n",
        "    if config.saveMetrics:\n",
        "        metricsBase = {\n",
        "            \"envSteps\": dreamer.totalEnvSteps,\n",
        "            \"gradientSteps\": dreamer.totalGradientSteps,\n",
        "            \"totalReward\": mostRecentScore if mostRecentScore is not None else float('nan')\n",
        "        }\n",
        "        # Merge all metrics dictionaries\n",
        "        all_metrics = {**metricsBase, **worldModelMetrics, **behaviorMetrics}\n",
        "        saveLossesToCSV(metricsFilename, all_metrics)\n",
        "\n",
        "        # Plot metrics (can be slow, consider doing it less frequently if performance is an issue)\n",
        "        if dreamer.totalGradientSteps % (config.checkpointInterval * 5) == 0: # Plot less frequently\n",
        "              plotMetrics(f\"{metricsFilename}\", savePath=f\"{plotFilename}\", title=f\"{config.environmentName} {config.robotName} - {config.runName}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Final plot\n",
        "if config.saveMetrics:\n",
        "    plotMetrics(f\"{metricsFilename}\", savePath=f\"{plotFilename}\", title=f\"{config.environmentName} {config.robotName} - {config.runName} (Final)\")\n",
        "env.close()\n",
        "envEvaluation.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "0Gr8XQGZy2r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sync with colab\n",
        "!git config --global user.email \"aman.chulawala@gmail.com\"\n",
        "!git add .\n",
        "!git commit -m \"Colab Push -> Added Image Wrapper for robosuite\"\n",
        "!git push https://<PAT>@github.com/FarStryke21/NaturalDreamer.git"
      ],
      "metadata": {
        "id": "cf7VK6qq1CAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRBTqoOQ4gsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}